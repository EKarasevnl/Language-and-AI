{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e51d30f7"
      },
      "outputs": [],
      "source": [
        "# INSTALL IF NEEDED:\n",
        "\n",
        "# !pip install spacy"
      ],
      "id": "e51d30f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa5Ci0q_bF6o"
      },
      "outputs": [],
      "source": [
        "# CONNECT TO COLAB IF NEEDED:\n",
        "\n",
        "# from google.colab import drive\n",
        "# import os\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "# os.chdir('./drive/MyDrive/data/tue_lai')"
      ],
      "id": "xa5Ci0q_bF6o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e090ae22"
      },
      "source": [
        "# Load the data"
      ],
      "id": "e090ae22"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecb07990"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_path = 'political_leaning.csv'\n",
        "data = pd.read_csv(data_path)"
      ],
      "id": "ecb07990"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1z_4JdC1e8i"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TARGET_COL = 'political_leaning'\n",
        "INDEPENDENT_COL = 'post'\n",
        "\n",
        "def label_encode(df, col_name):\n",
        "    label_encoder = LabelEncoder()\n",
        "    df[col_name] = label_encoder.fit_transform(df[col_name])\n",
        "    return df, label_encoder\n",
        "\n",
        "\n",
        "df, le = label_encode(data, TARGET_COL)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[INDEPENDENT_COL], df[TARGET_COL])"
      ],
      "id": "v1z_4JdC1e8i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2054c6c"
      },
      "source": [
        "### Create preprocessing pipelines"
      ],
      "id": "f2054c6c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff8ea73a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "CENTRALITY = TfidfVectorizer()\n",
        "CORPUS_AVG_TFIDF = CENTRALITY.fit_transform(data['post'].tolist()).mean(axis=0)"
      ],
      "id": "ff8ea73a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a20f966"
      },
      "outputs": [],
      "source": [
        "def scale_array(array):\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(np.array(array).reshape(-1,1))\n",
        "\n",
        "\n",
        "class StylometricFeatures(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A class to extract the following stylometric features:\n",
        "    1. avg sentence/token lengths,\n",
        "    2. avg stopwords ratio,\n",
        "    3. frequencies of POS tags.\n",
        "\n",
        "    By defeault it doesn't use tfidf, but there is a parameter to do so.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, include_tfidf=False):\n",
        "        self.include_tfidf = include_tfidf\n",
        "\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def tfidf_centrality(self, text):\n",
        "        text_matrix = CENTRALITY.transform(text)\n",
        "        text_tf = text_matrix.sum(axis=0)\n",
        "        result = np.sum((text_tf - CORPUS_AVG_TFIDF))\n",
        "        return result\n",
        "\n",
        "\n",
        "    def stop_word_ratio(self, post):\n",
        "        return len([word for word in post if word in STOP_WORDS]) / len(post)\n",
        "\n",
        "    def avg_word_length(self, post):\n",
        "        return sum(len(word) for word in post) / len(post)\n",
        "\n",
        "    def avg_sent_length(self, post):\n",
        "        return sum(len(sent) for sent in post) / len(post)\n",
        "\n",
        "    def part_of_speech_distribution(self, text):\n",
        "        words = word_tokenize(text)\n",
        "        pos_tags = pos_tag(words)\n",
        "        pos_freq = FreqDist(tag for word, tag in pos_tags)\n",
        "        top_tags = pos_freq.most_common(20)\n",
        "        total_tags = sum(pos_freq.values())\n",
        "        pos_distribution = {tag: freq / total_tags for tag, freq in top_tags}\n",
        "        vals = list(pos_distribution.values())\n",
        "        padded = vals + [0.0] * (20 - len(vals)) # for varied length texts\n",
        "        return padded\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Add temporary vars\n",
        "        sentences = [sent_tokenize(text) for text in X]\n",
        "        words = [word_tokenize(text) for text in X]\n",
        "\n",
        "        # Calculate stylometric features\n",
        "        stylometric_features = []\n",
        "\n",
        "        avg_sent_length = scale_array([self.avg_word_length(post) for post in sentences])\n",
        "        stylometric_features.append(avg_sent_length)\n",
        "\n",
        "        avg_word_length = scale_array([self.avg_word_length(post) for post in words])\n",
        "        stylometric_features.append(avg_word_length)\n",
        "\n",
        "        stopwords_ratio = scale_array([self.stop_word_ratio(post) for post in words])\n",
        "        stylometric_features.append(stopwords_ratio)\n",
        "\n",
        "        pos_dists = [self.part_of_speech_distribution(text) for text in X]\n",
        "\n",
        "        if self.include_tfidf:\n",
        "            tfidf_val = scale_array([self.tfidf_centrality(x) for x in words])\n",
        "            stylometric_features.append(tfidf_val)\n",
        "\n",
        "        # List of cols --> list of rows\n",
        "        stylometric_features = list(map(list, zip(*stylometric_features)))\n",
        "        stylometric_features = [[float(val) for val in row] for row in stylometric_features]\n",
        "\n",
        "        feats = [\n",
        "            style_vals + pos_vals\n",
        "            for style_vals, pos_vals in zip(\n",
        "                stylometric_features,\n",
        "                pos_dists\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        return np.array(feats)"
      ],
      "id": "3a20f966"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac7328e0"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "SPACY = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "class PosFeatures(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A class to extract POS features: for each post returns a list of corresponding POS tags\n",
        "    \"\"\"\n",
        "    def __init__(self, include_tfidf=False, length_percentile=95):\n",
        "        self.include_tfidf = include_tfidf\n",
        "        self.length_percentile = length_percentile\n",
        "        self._standartization_factor = 0\n",
        "\n",
        "    def transform(self, X, *_):\n",
        "        assert (self.sentence_size is not None), \"Fitting required\"\n",
        "\n",
        "        # Create the output matrix\n",
        "        result = np.zeros((len(X), self.sentence_size), dtype='uint8')\n",
        "\n",
        "        for i, x in enumerate(SPACY.pipe(X, batch_size=50)):\n",
        "            # Store the POS-tags\n",
        "            tags = np.fromiter((token.pos for token in x), dtype='uint8', count=len(x))\n",
        "\n",
        "            # Pad and truncate data, if necessary, and store them in result\n",
        "            last_index = len(tags) if len(tags) < self.sentence_size else self.sentence_size\n",
        "            result[i, :last_index] = tags[:last_index]\n",
        "\n",
        "        # Generate the factor one time to ensure applying the same factor at the next transformations\n",
        "        if self._standartization_factor == 0:\n",
        "            self._standartization_factor = np.min(result[result != 0]) - 1\n",
        "\n",
        "        # Standartize all valid elements to count from 1\n",
        "        result[result != 0] -= self._standartization_factor\n",
        "        return result\n",
        "\n",
        "    def fit(self, X, *_):\n",
        "        # Define an optimal sentence size covering a specific percent of all sample\n",
        "        self.sentence_size = int(np.percentile([len(t.split()) for t in X], self.length_percentile))\n",
        "        return self\n",
        "\n",
        "    def fit_transform(self, X, *_):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)"
      ],
      "id": "ac7328e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e32fdb11"
      },
      "source": [
        "# Create custom classifiers"
      ],
      "id": "e32fdb11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5daa2fdc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, rnn_type, hidden_size, output_size, input_size, dropout, num_layers, bidirectional):\n",
        "        super().__init__()\n",
        "        assert (rnn_type in ['gru', 'lstm', 'simple']), \"Invalid RNN type\"\n",
        "\n",
        "        rnn_params = {\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"input_size\": input_size,\n",
        "            \"dropout\": dropout,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"bidirectional\": bidirectional\n",
        "        }\n",
        "\n",
        "        if rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(**rnn_params)\n",
        "        elif rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(**rnn_params)\n",
        "        else:\n",
        "            self.rnn = nn.RNN(**rnn_params)\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.fc = nn.Linear(\n",
        "            (1 + bidirectional) * hidden_size,\n",
        "            output_size\n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        batch_size, seq_len, input_size = input_seq.shape\n",
        "\n",
        "        if self.rnn_type == 'lstm':\n",
        "            rnn_output, (last_hidden_state, last_cell_state) = self.rnn(input_seq)\n",
        "        else:\n",
        "            rnn_output, last_hidden_stwate = self.rnn(input_seq)\n",
        "\n",
        "        last_output = rnn_output[:, -1, :]\n",
        "        output = self.fc(last_output).view(batch_size, -1)\n",
        "        output_probs = self.softmax(output)\n",
        "        return output_probs\n"
      ],
      "id": "5daa2fdc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBz4jNvnuYHm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class RnnSklearnWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self,\n",
        "                 batch_size=32,\n",
        "                 epochs=3,\n",
        "                 dropout=0,\n",
        "                 rnn_type='gru',\n",
        "                 hidden_size=300,\n",
        "                 num_layers=1,\n",
        "                 bidirectional=False):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.dropout = dropout\n",
        "        self.rnn_type = rnn_type\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self._model = None\n",
        "\n",
        "    def fit(self, X, Y=None):\n",
        "        assert (Y is not None), \"Y is required\"\n",
        "        self.num_tags = np.max(X) + 1\n",
        "\n",
        "        rnn_params = {\n",
        "            \"rnn_type\": self.rnn_type,\n",
        "            \"input_size\": self.num_tags,\n",
        "            \"output_size\": np.max(Y) + 1,\n",
        "            \"hidden_size\": self.hidden_size,\n",
        "            \"dropout\": self.dropout,\n",
        "            \"num_layers\": self.num_layers,\n",
        "            \"bidirectional\": self.bidirectional\n",
        "        }\n",
        "        self.model = RNNClassifier(**rnn_params).to(DEVICE)\n",
        "        self.model.train()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=0.9)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        step = 0\n",
        "        for _ in range(self.epochs):\n",
        "            running_loss = 0\n",
        "            for X_batch, y_batch in DataLoader(list(zip(X, Y)), batch_size=self.batch_size, shuffle=True):\n",
        "                X_batch, y_batch = self.one_hot_encode(X_batch, self.num_tags).float().to(DEVICE), y_batch.long().to(DEVICE)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                out = self.model(X_batch)\n",
        "                loss = criterion(out, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                step += 1\n",
        "                running_loss += loss.item()\n",
        "                if step % 10 == 0:\n",
        "                    last_loss = running_loss / 50\n",
        "                    print('Batch {} loss: {}'.format(step, last_loss))\n",
        "                    running_loss = 0.\n",
        "        return self\n",
        "\n",
        "    def predict(self, X, y=None):\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"Fitting required before prediction!\")\n",
        "\n",
        "        self.model.eval()\n",
        "        preds = []\n",
        "        for X_batch in DataLoader(X, batch_size=self.batch_size):\n",
        "            X_batch = self.one_hot_encode(X_batch, self.num_tags).float().to(DEVICE)\n",
        "            print(X_batch.shape)\n",
        "            output = self.model.forward(X_batch)\n",
        "            preds.append(output)\n",
        "\n",
        "        preds = torch.cat(preds, dim=0).cpu().detach().numpy()\n",
        "        return np.argmax(preds, axis=1)\n",
        "\n",
        "\n",
        "    def one_hot_encode(self, tensor, num_classes):\n",
        "        batch_size, seq_len = tensor.size()\n",
        "        eye = torch.eye(num_classes)\n",
        "        one_hot_encoded = eye[tensor.long()]\n",
        "        one_hot_encoded = one_hot_encoded.view(batch_size, seq_len, num_classes)\n",
        "        return one_hot_encoded"
      ],
      "id": "lBz4jNvnuYHm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c34332e"
      },
      "source": [
        "### Evaluate"
      ],
      "id": "8c34332e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc5c0743"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_clf(clf, X, y_true, classes, normalize=True, cmap=plt.cm.Blues):\n",
        "    y_pred = clf.predict(X)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        title = 'Normalized Confusion Matrix'\n",
        "    else:\n",
        "        title = 'Confusion Matrix'\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "fc5c0743"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa6322fd"
      },
      "source": [
        "# Training & evaluating"
      ],
      "id": "aa6322fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f6fb783"
      },
      "source": [
        "### Baseline: TFIDF with SVM"
      ],
      "id": "4f6fb783"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7d32621"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC()\n",
        "\n",
        "\n",
        "clf_base = Pipeline([\n",
        "    ('features', StylometricFeatures()),\n",
        "    ('classifier', svm)\n",
        "])"
      ],
      "id": "c7d32621"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdba503b"
      },
      "outputs": [],
      "source": [
        "clf_base.fit(X_train, y_train);"
      ],
      "id": "bdba503b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1153f10f"
      },
      "outputs": [],
      "source": [
        "evaluate_clf(clf_base, X_test, y_test, classes=le.classes_)"
      ],
      "id": "1153f10f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c58e9a9"
      },
      "source": [
        "### Pure stylometry 1: StylometryFeatrues + RandomForest"
      ],
      "id": "8c58e9a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41373e43"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    max_depth=2, random_state=0\n",
        ")\n",
        "\n",
        "clf_style_rf = Pipeline([\n",
        "    ('features',  StylometricFeatures()),\n",
        "    ('classifier', rf)\n",
        "])"
      ],
      "id": "41373e43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf9f8af5"
      },
      "outputs": [],
      "source": [
        "clf_style_rf.fit(X_train, y_train);"
      ],
      "id": "cf9f8af5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "607cd763"
      },
      "outputs": [],
      "source": [
        "evaluate_clf(clf_style_rf, X_test, y_test, classes=le.classes_)"
      ],
      "id": "607cd763"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "583941c6"
      },
      "source": [
        "### Pure stylometry 2.0: PosFeatrues + RandomForest"
      ],
      "id": "583941c6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac47ced5"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(\n",
        "    max_depth=2, random_state=0\n",
        ")\n",
        "\n",
        "clf_pos_rf = Pipeline([\n",
        "        ('pre', PosFeatures()),\n",
        "        ('rf', rf)\n",
        "])"
      ],
      "id": "ac47ced5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c5559bc"
      },
      "outputs": [],
      "source": [
        "clf_pos_rf.fit(X_train, y_train);"
      ],
      "id": "2c5559bc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "829d7560"
      },
      "outputs": [],
      "source": [
        "evaluate_clf(clf_pos_rf, X_test, y_test, classes=le.classes_)"
      ],
      "id": "829d7560"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95530ca4"
      },
      "source": [
        "### Pure stylometry 2.1: PosFeatrues + RNN"
      ],
      "id": "95530ca4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e2ceb4a"
      },
      "outputs": [],
      "source": [
        "rnn = RnnSklearnWrapper(\n",
        "    epochs=5,\n",
        "    rnn_type='gru',\n",
        "    dropout=0.3,\n",
        "    num_layers=3,\n",
        "    bidirectional=True\n",
        ")\n",
        "\n",
        "\n",
        "clf_pos_rnn = Pipeline([\n",
        "        ('pre', PosFeatures()),\n",
        "        ('rnn', rnn)\n",
        "])"
      ],
      "id": "0e2ceb4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8a9b0f4"
      },
      "outputs": [],
      "source": [
        "clf_pos_rnn.fit(X_train, y_train);"
      ],
      "id": "f8a9b0f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b436e46"
      },
      "outputs": [],
      "source": [
        "evaluate_clf(clf_pos_rnn, X_test, y_test, classes=le.classes_)"
      ],
      "id": "4b436e46"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17634e53"
      },
      "source": [
        "### Stylometry with TFIDF 1:  StylometryFeatrues + RandomForest"
      ],
      "id": "17634e53"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8df6cb67"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    max_depth=2, random_state=0\n",
        ")\n",
        "\n",
        "clf_style_rf = Pipeline([\n",
        "    ('features', StylometricFeatures(include_tfidf=True)),\n",
        "    ('classifier', rf)\n",
        "])"
      ],
      "id": "8df6cb67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64a2b253"
      },
      "outputs": [],
      "source": [
        "clf_style_rf.fit(X_train, y_train);"
      ],
      "id": "64a2b253"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce23f251"
      },
      "outputs": [],
      "source": [
        "evaluate_clf(clf_style_rf, X_test, y_test, classes=le.classes_)"
      ],
      "id": "ce23f251"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9Pe9xEIhZhm"
      },
      "outputs": [],
      "source": [],
      "id": "d9Pe9xEIhZhm"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e090ae22",
        "e32fdb11",
        "4f6fb783",
        "8c58e9a9",
        "583941c6",
        "17634e53"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}